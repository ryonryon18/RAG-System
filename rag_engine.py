# rag_lab/rag_engine.py

import os
from dotenv import load_dotenv
from openai import OpenAI
from retrievers.topk import retrieve_top_k
from retrievers import get_retriever

# APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

PROMPT_TEMPLATE = """
ã‚ãªãŸã¯çŸ¥è­˜è±Šå¯ŒãªåŒ»ç™‚ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å°‚é–€çš„ã§ã‚ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã®å›ç­”ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

### å‚è€ƒæƒ…å ±:
{context}

### è³ªå•:
{question}

### å›ç­”:
"""

# ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¾ã¨ã‚ã‚‹é–¢æ•°
def build_prompt(question, context_chunks):
    context_text = "\n\n".join(chunk["text"] for chunk in context_chunks)
    return PROMPT_TEMPLATE.format(context=context_text, question=question)

# å®Ÿè¡Œï¼šRetrieve â†’ Prompt â†’ GPTç”Ÿæˆ
def generate_answer(question, retriever_method="topk", k=3):
    retriever = get_retriever(retriever_method)
    context_chunks = retriever(question, k=k)
    prompt = build_prompt(question, context_chunks)

    response = client.chat.completions.create(
        model="gpt-4",  # å¿…è¦ã«å¿œã˜ã¦ gpt-3.5 ã«å¤‰æ›´OK
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return response.choices[0].message.content.strip()

# å‹•ä½œç¢ºèªï¼ˆç›´æ¥å®Ÿè¡Œæ™‚ï¼‰
if __name__ == "__main__":
    question = "RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    answer = generate_answer(question)
    print("ğŸ§  å›ç­”:\n", answer)
